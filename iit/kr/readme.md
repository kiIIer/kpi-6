# Контрольна робота з Інфраструктура інформаційних технологій

## Завдання

Інфраструктура для системи відеоспостереження на дорогах, яка є повністю автоматизованою і забезпечує збереження всіх зібраних даних. Кількість запитів до системи становить ~50000 на секунду. Також враховані можливі аварійні ситуації для забезпечення надійності та безпеки системи.

## Аналіз завдання

В завданні вказано, що система повинна бути повністю автоматизованою і забезпечувати збереження всіх даних. Тобто з мінімальними втручаннями людини працювати, та зберігати відео дані. Кількість запитів 50 тис. на секунду, це доволі багато навіть для простих запитів до REST сервісів, а у нас робота з відео, давайте проаналізуємо запити та що в них буде.

### Аналіз запитів

У нас відеоспостереження на дорогах, тому ці запити будуть запити з машинами на дорогах. Така велика кількість запитів може значити що в нас дуже багато камер які періодично надсилають зібрані дані. Припустимо що камери працюють 24/7 та кожну хвилину надсилають записану інформацію.

Тоді виходить що при 50 тис. запитів на секнуду в нас буде приблизно 600 тис. камер, кожна з яких надсилає запит кожну хвилину. Припустимо що в нас нема ніякого аналізу що на цих відео буде, і нам потрібно просто зберігати їх у тому ж форматі що і надійшли. Умовно ми будемо мати файл номерКамери_дата_час.mp4.

Проаналізуємо якість відео, нехай в нас відео з 1 кадром на секунду та з мінімальною якістю, типу 640x480. Одна хвилина такого відео за моїми розрахунками буде важити 27 мегабайт. Це доволі мало, але в нас 600 тис камер, тобто це приблизно 16 гігабайт за хвилину, що неймовірно велика кількість даних. За день роботи це 23328 терабайт.

Після підрахунків в AWS Calculator, це виходить 13 мільйонів долларів за рік для зберігання цих даних та цих запитів. Скоріше за все я просто неправильно зрозумів завдання, бо так багато даних ніде не вийде зребегти.

Припустимо, що ці запити - прості запити з, наприклад, кількістю машин кожну секунду, які бачить камера, тоді це виходить що в нас 50 тис. камер, які щосекунди надсилають кількість машин, що вони бачать. Тоді в нас вийде хоча б уявити таку кількість даних.

Це знову будуть gRPC запити від камер, які будуть числом вказувати кількість машин, і нам їх потрібно буде зберегти.

### Аналіз надійності та безпеки

Припустимо, що ця система камер в Україні, тоді в нас буде виликий шанс відключення світла в серверного центра, або офісу компанії, та ніхто не виключав снаряди та влучання, що може повністю зруйнувати інфраструктуру.

Тому оптимальним буде рішення розмістити інфрастуктуру за межами України, або використати хмарного провайдера, де ми зможемо задати регіон.

Щодо безпеки, оскільки в нас система збирання даних, я не думаю, що її будуть використовувати для отримання цих даних. Скоріше за все вони будуть використовуватись для статистики чи потім для машинного навчання, тобто в системи немає потреби надавати ці данні, тільки збирати, тому нам не потрібно мати акаути адміністраторів і всяке таке, бо наш застосунок не буде взаємодіяти з користувачами.

Оскільки інформація про кількість машин на дорозі не дуже приватна та на потребує надзвичайної безпеки, звичайне шифрування за допомогою сертифікату TSL, буде достатнім для забезпечення необхідної безпеки.

Але можлива ситуація, коли сторонні люди та хакери спробують "прикинутись" іншими камерами. Розгляньмо це детальніше. Оскільки камера - це не користувач, ми не можемо просто створити аккаунт для камери та зробити так, щоб камера там реєструвалась, нам необхідно зробити спосіб ідентифікувати наші камери, та не допускати сторонніх даних в нашу систему.

### Аналіз компанії

Оскільки не вказано компанію яка це використовує, я припущу що це або мала компанія, або студенти, що роблять дослідження, тобто малий бізнес без свого офісу.

## Побудова серверної інфрастуктури

### Клаудне рішення

Оскільки в нас мала компанія без офісу, я вважаю, що найкращим рішенням буде використання клауд провайдера для створення інфрастуктури. Для цього завадання оберемо AWS, вони надають IaaS (infrastructure as a service), що дозволить нам створити нашу інфрастуктуру в клауді.

Цей вибір є оптимальним для нас, бо нам не потрібно робити капітальні витрати на закупівлю серверного обладнання, також нам не потрібно буде чекати довго для отримання та налаштування всього обладнання, ми зможемо повністю створити інфрастуктуру за можливо годину і одразу її використовувати.

Також це вирішує нашу проблему з надійністю, бо ми зможемо розмітити інфраструктуру за межами України та в різних AZ (availability zone), що дозволить нашому сервісу працювати навіть коли пів регіону вийде з ладу.

### Опис Virtual Private Cloud

Для цього проетку, необхідно створити VPC (virtual private cloud), не ніби умовне угрупування всередині AWS, що дозволить відокремити всі ресурси, що належать цій інфрастуктурі від можливо інших інфрастуктур цієї компанії, які не відносяться до збору даних про кількість машин на дорогах.

Під час створення EKS кластеру, сама control panel сториться в окремому VPC, який належить AWS та ми не можемо ним керувати. Це серце нашого кластеру, саме ця контрол панель буде керувати всім іншим.

Але нам все одно необхідно створити наші підмережі для worker нодів. Загалом кластер вимагає викорстання хоча б двох AZ (availability zones), але для більшої надійності ми використаємо більше.

В кожній AZ, ми створимо 2 підмережі, одну публічну та одну приватну. Приватна мережа буде використовуватись для розгортання саме нодів кластера де буде наш застосунок, але до них не буде доступу ззовні нашої vpc, тому kubernetes для нас автоматично розгорне load balancer у публічній мережі та буде передавати трафік до оптимального ноду.

Доступ до cluster endpoint-у буде приватним. Це значить, що до контрол панелі кластеру можна будет потрапити тільки зсередини vpc, це ще більше посилить приватність та безпеку кластеру. В приватному режимі роботи ендпоінту, весь трафік буде надхотити тільки зсередини vpc, за допомогою X-ENI, які будуть забезпечувати комунікацію між моєю vpc та vpc від aws.

Після виконання скріпту, кожен нод буде намагатись досягти ендпоінт кластеру за допомогою відповідного X-ENI, та зареєструватись. Нод отримає інструкції від ендпоінту, буде надсилати туди hearbeat сигнал та виконувати його вказівки.

Для простоти, ми будемо використовувати IPv4 за замовчуванням, та в VPC задамо 10.0.0.0/16 як CIDR (Classless Inter-Domain Routing) блок. Оскільки в нас нема так багато різних VPC, в нас не будуть проблеми з початком адреси (10.0) та маска у 16, надасть 65000 ip адресів для речей всередині vpc, що дозволить нам мати по суті необмежену кількість вільних адресів(ми ніколи не наберем 65 тисяч). Але важливо що в нас буде запас, щоб в нас не стались проблеми коли будемо розширюватись горизонтально(scale out).

Насупне, створити CIDR для підмереж, ми використаємо 3 AZ всередині регіону eu-north-1, по 2 підмережі, і ми отримуємо 6 необхідних підмежер для створення. Для них необхідно створити свої CIDR блоки, ми могли б збільшити маску на 3 для них, та мати блоки типу 10.0.0.0/19, 10.0.32.0/19, 10.0.64.0/19, але мати всього 2 вільних адреси для інших підмереж, це трохи страшно, бо при найменшому розширенні системи, в нас закінчаться адреси, тому візьмемо маску 20, щоб було достатньо місця і для можливих додаткових підмереж, та 4096 адрес для нодів кластеру буде предостатньо.
